{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Algorithms_Data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "data = data[-15000:]  # Training on more than this doesn't seem to improve results, but takes longer\n",
    "targets = data['Win']\n",
    "data = data[['fran_elo', 'opp_elo', 'RollAvg_A_5_pts', 'RollAvg_B_5_pts', 'RollAvg_A_5_opp_pts', 'RollAvg_B_5_opp_pts', 'Days_Since_Last']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fran_elo</th>\n",
       "      <th>opp_elo</th>\n",
       "      <th>RollAvg_A_5_pts</th>\n",
       "      <th>RollAvg_B_5_pts</th>\n",
       "      <th>RollAvg_A_5_opp_pts</th>\n",
       "      <th>RollAvg_B_5_opp_pts</th>\n",
       "      <th>Days_Since_Last</th>\n",
       "      <th>Win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32530</th>\n",
       "      <td>1489.768214</td>\n",
       "      <td>1684.042297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>95.2</td>\n",
       "      <td>96.2</td>\n",
       "      <td>86.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32531</th>\n",
       "      <td>1569.508971</td>\n",
       "      <td>1558.067109</td>\n",
       "      <td>83.8</td>\n",
       "      <td>93.2</td>\n",
       "      <td>77.4</td>\n",
       "      <td>84.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32532</th>\n",
       "      <td>1687.615767</td>\n",
       "      <td>1528.192875</td>\n",
       "      <td>107.2</td>\n",
       "      <td>98.2</td>\n",
       "      <td>94.2</td>\n",
       "      <td>92.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32533</th>\n",
       "      <td>1509.910406</td>\n",
       "      <td>1448.525491</td>\n",
       "      <td>101.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>106.8</td>\n",
       "      <td>97.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32534</th>\n",
       "      <td>1456.563819</td>\n",
       "      <td>1571.171564</td>\n",
       "      <td>97.4</td>\n",
       "      <td>93.6</td>\n",
       "      <td>91.2</td>\n",
       "      <td>83.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fran_elo      opp_elo  RollAvg_A_5_pts  RollAvg_B_5_pts  \\\n",
       "32530  1489.768214  1684.042297             91.8             95.2   \n",
       "32531  1569.508971  1558.067109             83.8             93.2   \n",
       "32532  1687.615767  1528.192875            107.2             98.2   \n",
       "32533  1509.910406  1448.525491            101.2            100.0   \n",
       "32534  1456.563819  1571.171564             97.4             93.6   \n",
       "\n",
       "       RollAvg_A_5_opp_pts  RollAvg_B_5_opp_pts  Days_Since_Last    Win  \n",
       "32530                 96.2                 86.8              1.0  False  \n",
       "32531                 77.4                 84.4              1.0  False  \n",
       "32532                 94.2                 92.2              1.0  False  \n",
       "32533                106.8                 97.6              1.0  False  \n",
       "32534                 91.2                 83.6              3.0  False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([data.head(), targets.head()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above table is a sample of the data we will be using in order to predict whether the home team will win an NBA basketball game. Each row represents one match up between two basketball teams. Team A is the away team, team B is the home team.  Note a value of False in the Win column implies that team B (the home team) won.\n",
    "\n",
    "The ELO rating system is a method for calculating the relative skill levels of players in two-player games such as chess. We have adapted this system in order to measure skill levels of different NBA teams based on their recent performance.  We computed the ELO ratings for each team over the history of our dataset, and we use this as a feature.\n",
    "\n",
    "A_elo: The ELO ranking for the away team (A) at the time this game was played.\n",
    "\n",
    "B_elo: The ELO ranking for the home team (B) at the time this game was played.\n",
    "\n",
    "RollAvg_A_5_pts: For the away team, the average points this specific team has scored in their last 5 games.\n",
    "\n",
    "RollAvg_B_5_pts: For the home team, the average points this specific team has scored in their last 5 games.\n",
    "\n",
    "RollAvg_A_5_opp_pts: For the home team, the average points this specific team has allowed in their last 5 games.\n",
    "\n",
    "RollAvg_B_5_opp_pts: For the away team, the average points this specific team has allowed in their last 5 games.\n",
    "\n",
    "Days_Since_Last: Number of days since the away team last played a game.\n",
    "\n",
    "For more information on the ELO scores, see our Machine Learning page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runAlgorithm(model, trials=10,):\n",
    "    \"\"\" Run a given machine learning algorithm on globally defined data.\n",
    "        Splits data randomly into test and training sets.\n",
    "        Returns the average time and accuracy.\n",
    "    \n",
    "    :param model: A sklearn model initialized with parameters.\n",
    "    :param trials: Number of trials to run.  Defaults to 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    preds =[]\n",
    "    for t in range(trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "        t1.append(time.time())\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        preds.append((model.predict(X_test)>.5) == y_test)\n",
    "        t2.append(time.time())\n",
    "        \n",
    "    return np.mean(preds),np.mean(np.array(t2)-np.array(t1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of ML algorithms\n",
    "\n",
    "## Baseline\n",
    "Guess that the home team wins every game.  If a method doesn't do better than this, it isn't learning much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.599266666667\n",
      "Time: 0.000494384765625\n"
     ]
    }
   ],
   "source": [
    "t1 = []\n",
    "t2 = []\n",
    "preds = []\n",
    "for t in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    t1.append(time.time())\n",
    "\n",
    "    preds.append((np.zeros(len(y_test)) == y_test).mean())\n",
    "    t2.append(time.time())\n",
    "\n",
    "baseline_ = np.mean(preds), np.mean(np.array(t2)-np.array(t1))\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*baseline_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66503030303\n",
      "Time: 3.12863096324\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalize data\n",
    "X = preprocessing.scale(X_train)\n",
    "\n",
    "accuracy = []\n",
    "times = []\n",
    "for i in xrange(11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    X = preprocessing.scale(X_train)\n",
    "    s = time.time()\n",
    "    SVM_model = svm.SVC(C=10**-5, kernel='poly', coef0=0, gamma=10**-2).fit(X, y_train)\n",
    "    preds = SVM_model.predict(X_test)\n",
    "    prob = np.sum(preds == y_test)/len(y_test)\n",
    "    times.append(time.time() - s)\n",
    "    accuracy.append(prob)\n",
    "\n",
    "svm_r = (np.mean(accuracy), np.mean(times))\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*svm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.759359333333\n",
      "Time: 0.02646393013\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "lam = 10**2\n",
    "\n",
    "model = lm.LogisticRegression(C=np.abs(1/lam))\n",
    "log_reg = runAlgorithm(model, trials=500)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.759647333333\n",
      "Time: 0.0337161097527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge as ri\n",
    "\n",
    "skRidge = ri()\n",
    "rid_reg = runAlgorithm(model,trials = 500)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*rid_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757948\n",
      "Time: 0.00586140775681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda_ = runAlgorithm(model,500)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*lda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.756080666667\n",
      "Time: 0.0041863694191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "qda_= runAlgorithm(model, 500)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*qda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.741983333333\n",
      "Time: 0.904680302143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "\n",
    "#start = time.time()\n",
    "gbc = GBC(max_leaf_nodes=500, min_weight_fraction_leaf=0.001, min_samples_split=100, learning_rate=.4, max_features=\"auto\")\n",
    "grad = runAlgorithm(gbc, trials=100)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.732933333333\n",
      "Time: 6.04253869057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "\n",
    "start = time.time()\n",
    "gbm = mlp(solver=\"lbfgs\", activation='tanh', tol=1e-4, alpha=1e-5)#.fit(X_train,Y_train)\n",
    "mlp_r = runAlgorithm(gbm, trials=10)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*mlp_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.743133333333\n",
      "Time: 1.74112932682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "rfc = RFC(min_samples_split=3, n_estimators =100)\n",
    "forest = runAlgorithm(rfc, trials=10)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.663857333333\n",
      "Time: 0.0861741452217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "dtc = DTC(criterion='entropy')\n",
    "tree = runAlgorithm(dtc, trials=500)\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/adammorrow1/anaconda/lib/python2.7/site-packages/libxgboostwrapper.so, 6): Library not loaded: @rpath/./libgomp.1.dylib\n  Referenced from: /Users/adammorrow1/anaconda/lib/python2.7/site-packages/libxgboostwrapper.so\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7ff9b21840aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adammorrow1/anaconda/lib/python2.7/site-packages/xgboost.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mxglib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_xglib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adammorrow1/anaconda/lib/python2.7/site-packages/xgboost.py\u001b[0m in \u001b[0;36mload_xglib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdll_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostLibraryNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot find find the files in the candicate path '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdll_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# DMatrix functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adammorrow1/anaconda/lib/python2.7/ctypes/__init__.pyc\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adammorrow1/anaconda/lib/python2.7/ctypes/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/adammorrow1/anaconda/lib/python2.7/site-packages/libxgboostwrapper.so, 6): Library not loaded: @rpath/./libgomp.1.dylib\n  Referenced from: /Users/adammorrow1/anaconda/lib/python2.7/site-packages/libxgboostwrapper.so\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "accuracy = []\n",
    "times = []\n",
    "for i in xrange(500):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    start = time.time()\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    param = {'objective':'multi:softmax', 'num_class':5,'normalize_type':'forest','rate_drop':1, 'lambda':50, 'alpha':10}\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dtest) > .5\n",
    "    accuracy.append((preds == y_test).mean())\n",
    "    times.append(time.time() -start)\n",
    "    #print preds\n",
    "\n",
    "xgb_r = (np.mean(accuracy), np.mean(times))\n",
    "print 'Accuracy: {}\\nTime: {}'.format(*xgb_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-147ac313e862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                       'MLP', 'Decision Tree', 'Random Forest', 'LDA', 'QDA']\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbaseline_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrid_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcomps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcomps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_r' is not defined"
     ]
    }
   ],
   "source": [
    "comps = pd.DataFrame(columns=['Algorithm', 'Accuracy', 'Time'])\n",
    "comps['Algorithm'] = ['Baseline', 'SVM', 'Logistic Regression', 'Ridge Regression', \n",
    "                      'Gradient Boost', 'XGBoost',\n",
    "                      'MLP', 'Decision Tree', 'Random Forest', 'LDA', 'QDA']\n",
    "\n",
    "results = [baseline_, svm_r, log_reg, rid_reg, grad, xgb_r, mlp_r, tree, forest, lda_, qda_]\n",
    "comps['Accuracy'] = [m[0] for m in results]\n",
    "comps['Time'] = [m[1] for m in results]\n",
    "comps.sort_values('Accuracy', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>QDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Algorithm Accuracy Time\n",
       "0              Baseline      NaN  NaN\n",
       "1                   SVM      NaN  NaN\n",
       "2   Logistic Regression      NaN  NaN\n",
       "3      Ridge Regression      NaN  NaN\n",
       "4        Gradient Boost      NaN  NaN\n",
       "5               XGBoost      NaN  NaN\n",
       "6                   MLP      NaN  NaN\n",
       "7         Decision Tree      NaN  NaN\n",
       "8         Random Forest      NaN  NaN\n",
       "9                   LDA      NaN  NaN\n",
       "10                  QDA      NaN  NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps.sort_values('Time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy and effectiveness of the methods you used and compare the relative strengths and weaknesses of each method for this particular project.  \n",
    "\n",
    "The methods that we focused on were regression based methods, tree based methods, neural nets, and SVMs. \n",
    "\n",
    "### Regression based\n",
    "Methods used: Logistic Regression, Ridge Regression\n",
    "\n",
    "Both methods worked extremely well, with accuracies around 76% and running in only ~.03 seconds.  Note that logistic regression is really just a classifier, meaning the output is 1 or 0.  However, ridge regression outputs a continuous variable. Since a regression algorithm returns a continuous output, we set a threshold value of .5.  In other words, if the model returns a number greater than .5, we classified this as 1 or True.\n",
    "\n",
    "### Tree based\n",
    "Methods used: Decision Tree, Random Forest, Gradient Boost, XGBoost\n",
    "\n",
    "Random Forests ran about as accurately as some of our better performers at 74%, but is a much slower method running in about a second and a half.\n",
    "\n",
    "As expected, Decision Trees ran less accurately than Random Forests at 67% but runs much faster at ~.1 seconds. Decision Trees are one of the fasted methods we tested.\n",
    "\n",
    "XGBoost was among the best of the tree methods, both fast and respectably accurate.  However, we could not quite get it to surpass the regression methods in accuracy, and those take only half the time.\n",
    "\n",
    "Gradient Boost performed just under XGBoost both in time and accuracy.\n",
    "\n",
    "\n",
    "### Gaussian Discriminant Analysis\n",
    "Methods used: Linear Discriminant Analysis, Quadratic Discriminant Analysis\n",
    "\n",
    "Despite their simplicity and the lack of parameters to tune, these two methods both achieve accuracy above 75% and run quickly. They are not quite as accurate as the regression methods, but they are about an order of magnitude faster. The fact that this works so well suggests that the wins and losses are somewhat linearly separable.\n",
    "\n",
    "\n",
    "### Neural nets\n",
    "Methods used: MLP\n",
    "\n",
    "Our Multi Layer Perception (MLP) was as accurate as many of our top performers at ~75%, but was by far our slowest algorithm taking nearly 7.5 seconds to run. As such even though it is accurate, we would not choose to continue working with it because it is so much slower than other algorithms that are equally accurate.\n",
    "\n",
    "### SVM\n",
    "Our Support Vector Machine ran the worst accuracy out of any of our other methods at 65%. It also was our second slowest methods running at nearly 3 seconds. Because it was outperformed by almost all of our other tests in both categories, we will not continue working with this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithms we chose not to use\n",
    "\n",
    "### Nearest neighbor\n",
    "\n",
    "Due to the fact that we are not performing any kind of cluster analysis, we have decided the NN-classifying and NN-regression algorithms are not useful. \n",
    "\n",
    "### Mixture models with latent variables (train with EM)\n",
    "\n",
    "Because there is not an unknown distribution in our dataset, mixture models along with EM will not work with our data set.\n",
    "\n",
    "\n",
    "### Kalman Filters\n",
    "\n",
    "Our data is not a true time series because we are not interested in how teams' overall performances changes over time. Games are decided strictly by winners and losers, so there is not any measurable error or noise. Because of this we have determined that it will not be useful in our methods. \n",
    "\n",
    "### AR, MA, ARMA, ARIMA time series models\n",
    "\n",
    "Because these models are trying to describe certain time-varying processes of a time series, they will not be helpful in trying to predict wins and losses of specific games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "Basketball game outcomes are very unreliable.  Even a team that has performed well for the last few games can suddenly and inexplicably lose to a team that has been performing poorly; and this happens not infrequently.  Due to these vagaries, we feel that 75% accuracy is quite reasonable for this stage of our project.  We hope to improve on this over the next month by including more features in our data, namely team statistics: free throws, shots attempted, steals, percentages, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR\n",
    "Epsilon-Support Vector Regression. Support Vector Regression (SVR) attempts to minimize the generalization error bound so as to achieve generalized performance. The idea of SVR is based on the computation of a linear regression function in a high dimensional feature space where the input data are mapped via a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matching Pursuit (OMP)\n",
    "A sparse approximation algorithm which involves finding the \"best matching\" projections of multidimensional data onto the span of an over-complete (i.e., redundant) dictionary D. The main difference from MP is that after every step, all the coefficients extracted so far are updated, by computing the orthogonal projection of the signal onto the set of atoms selected so far. This can lead to better results than standard MP, but requires more computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier\n",
    "\n",
    "An “extra trees” classifier, otherwise known as an “Extremely randomized trees” classifier, is a variant of a random forest. Unlike a random forest, at each step the entire sample is used and decision boundaries are picked at random, rather than the best one. In real world cases, performance is comparable to an ordinary random forest, sometimes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "A Voiting Classifier is used to employ multiple individual classifiers and combine their predictions, which could achieve better performance than a single classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
