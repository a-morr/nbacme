{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "filename = 'Algorithms_Data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "data = data[-15000:]\n",
    "targets = data['Win']\n",
    "data = data[['fran_elo', 'opp_elo', 'RollAvg_A_5_pts', 'RollAvg_B_5_pts', 'RollAvg_A_5_opp_pts', 'RollAvg_B_5_opp_pts', 'Days_Since_Last']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runAlgorithm(model, trials=10,):\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    preds =[]\n",
    "    for t in range(trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "        t1.append(time.time())\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        preds.append((model.predict(X_test)>.5) == y_test)\n",
    "        t2.append(time.time())\n",
    "        \n",
    "    return np.mean(preds),np.mean(np.array(t2)-np.array(t1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Guess that the home team wins every game.  If a method doesn't do better than this, it isn't learning much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59779999999999989, 0.00042645931243896485)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = []\n",
    "t2 = []\n",
    "preds = []\n",
    "for t in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    t1.append(time.time())\n",
    "\n",
    "    preds.append((np.zeros(len(y_test)) == y_test).mean())\n",
    "    t2.append(time.time())\n",
    "\n",
    "baseline_ = np.mean(preds), np.mean(np.array(t2)-np.array(t1))\n",
    "baseline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.66039393939393942, 3.9025016047737817)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalize data\n",
    "X = preprocessing.scale(X_train)\n",
    "\n",
    "accuracy = []\n",
    "times = []\n",
    "for i in xrange(11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    X = preprocessing.scale(X_train)\n",
    "    s = time.time()\n",
    "    SVM_model = svm.SVC(C=10**-5, kernel='poly', coef0=0, gamma=10**-2).fit(X, y_train)\n",
    "    preds = SVM_model.predict(X_test)\n",
    "    prob = np.sum(preds == y_test)/len(y_test)\n",
    "    times.append(time.time() - s)\n",
    "    accuracy.append(prob)\n",
    "\n",
    "svm_r = (np.mean(accuracy), np.mean(times))\n",
    "print svm_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75888666666666671, 0.046206696033477782)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "lam = 10**2\n",
    "\n",
    "model = lm.LogisticRegression(C=np.abs(1/lam))\n",
    "log_reg = runAlgorithm(model, trials=100)\n",
    "print log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75860000000000005, 0.086951708793640142)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge as ri\n",
    "\n",
    "skRidge = ri()\n",
    "rid_reg = runAlgorithm(model,trials = 10)\n",
    "print rid_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.74092666666666662, 0.76354182243347168)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "\n",
    "#start = time.time()\n",
    "gbc = GBC(max_leaf_nodes=500, min_weight_fraction_leaf=0.001, min_samples_split=100, learning_rate=.4, max_features=\"auto\")\n",
    "grad = runAlgorithm(gbc, trials=100)\n",
    "print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.74706666666666666, 5.2733932018280028)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "\n",
    "start = time.time()\n",
    "gbm = mlp(solver=\"lbfgs\", activation='tanh', tol=1e-4, alpha=1e-5)#.fit(X_train,Y_train)\n",
    "mlp_r = runAlgorithm(gbm, trials=10)\n",
    "print mlp_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7399, 1.6552239894866942)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "rfc = RFC(min_samples_split=3, n_estimators =100)\n",
    "forest = runAlgorithm(rfc, trials=10)\n",
    "print forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.66620000000000001, 0.11191978454589843)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "dtc = DTC(criterion='entropy')\n",
    "tree = runAlgorithm(dtc, trials=10)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.74051515151515157, 0.063956282355568619)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "accuracy = []\n",
    "times = []\n",
    "for i in xrange(11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=.2)\n",
    "    start = time.time()\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    bst = xgb.train({}, dtrain)\n",
    "    preds = bst.predict(dtest) > .5\n",
    "    accuracy.append((preds == y_test).mean())\n",
    "    times.append(time.time() -start)\n",
    "    #print preds\n",
    "\n",
    "xgb_r = (np.mean(accuracy), np.mean(times))\n",
    "print xgb_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithms we chose not to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Nearest neighbor\n",
    "\n",
    "Due to the fact that we are not performing any kind of cluster analysis, we have decided the NN-classifying and NN-regression algorithms are not useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian discriminant analysis\n",
    "\n",
    "Similar to the issues with nearest neighbor, since we are not doing any kind of classification, GDA will not be useful with our data set or our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture models with latent variables (train with EM)\n",
    "\n",
    "Because there is not an unknown distribution in our dataset, mixture models along with EM will not work with our data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filters\n",
    "\n",
    "Our data is not a true time series because we are not interested in how teams' overall performances changes over time. Games are decided strictly by winners and losers, so there is not any measurable error or noise. Because of this we have determined that it will not be useful in our methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR, MA, ARMA, ARIMA time series models\n",
    "\n",
    "Because these models are trying to describe certain time-varying processes of a time series, they will not be helpful in trying to predict wins and losses of specific games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.660394</td>\n",
       "      <td>3.902502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.758887</td>\n",
       "      <td>0.046207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>0.086952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.740927</td>\n",
       "      <td>0.763542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.740515</td>\n",
       "      <td>0.063956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>5.273393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.666200</td>\n",
       "      <td>0.111920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.739900</td>\n",
       "      <td>1.655224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Algorithm  Accuracy      Time\n",
       "0             Baseline  0.597800  0.000426\n",
       "1                  SVM  0.660394  3.902502\n",
       "2  Logistic Regression  0.758887  0.046207\n",
       "3     Ridge Regression  0.758600  0.086952\n",
       "4       Gradient Boost  0.740927  0.763542\n",
       "5              XGBoost  0.740515  0.063956\n",
       "6                  MLP  0.747067  5.273393\n",
       "7        Decision Tree  0.666200  0.111920\n",
       "8        Random Forest  0.739900  1.655224"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps = pd.DataFrame(columns=['Algorithm', 'Accuracy', 'Time'])\n",
    "comps['Algorithm'] = ['Baseline', 'SVM', 'Logistic Regression', 'Ridge Regression', \n",
    "                      'Gradient Boost', 'XGBoost',\n",
    "                      'MLP', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "results = [baseline_, svm_r, log_reg, rid_reg, grad, xgb_r, mlp_r, tree, forest]\n",
    "comps['Accuracy'] = [m[0] for m in results]\n",
    "comps['Time'] = [m[1] for m in results]\n",
    "comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
